name: å¤šè¿›ç¨‹æ›´æ–°jn950

on:
  workflow_dispatch:  # æ‰‹åŠ¨è§¦å‘
  schedule:
  # åŒ—äº¬æ—¶é—´å‡Œæ™¨2ç‚¹ = UTCæ—¶é—´å‰ä¸€å¤©18ç‚¹
  - cron: '0 2,10 * * *'

jobs:
  process:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests aiohttp aiodns
        
    - name: Process TV Channels (Multithreaded)
      run: |
        cat <<'EOF'>tv_processor.py
        import re
        import requests
        from urllib.parse import quote
        import sys
        import asyncio
        import aiohttp
        import time

        # ==================== é…ç½®å‚æ•° ====================
        SOURCE_URL = "https://raw.githubusercontent.com/jn950/live/refs/heads/main/tv/pllive.txt"
        LOGO_BASE_URL = "https://gitee.com/tyrian9905/iptv/raw/master/tvlogo/"
        EPG_URL = "https://gitee.com/taksssss/tv/raw/main/epg/51zmt.xml.gz"
        GROUP_FLAT = "JN950"

        # ==================== å¤šçº¿ç¨‹ä¼˜åŒ–é…ç½® ====================
        MAX_WORKERS = 30                     # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        CONNECT_TIMEOUT = 3                  # è¿æ¥è¶…æ—¶ï¼ˆç§’ï¼‰
        TOTAL_TIMEOUT = 8                    # æ€»è¶…æ—¶ï¼ˆç§’ï¼‰
        READ_TIMEOUT = 5                     # è¯»å–è¶…æ—¶ï¼ˆç§’ï¼‰
        BATCH_SIZE = 100                     # æ¯æ‰¹å¤„ç†æ•°é‡
        MAX_RETRIES = 2                      # æœ€å¤§é‡è¯•æ¬¡æ•°

        # æ›´å®½æ¾çš„HTTPçŠ¶æ€ç åˆ¤å®š
        VALID_STATUS_CODES = [200, 301, 302, 303, 307, 400, 403, 404]

        # ==================== CCTVæ˜¾ç¤ºåç§°æ˜ å°„ ====================
        CCTV_DISPLAY_NAMES = {
            "CCTV1": "CCTV-1 ç»¼åˆ",
            "CCTV2": "CCTV-2 è´¢ç»",
            "CCTV3": "CCTV-3 ç»¼è‰º",
            "CCTV4": "CCTV-4 ä¸­æ–‡å›½é™…",
            "CCTV5": "CCTV-5 ä½“è‚²",
            "CCTV6": "CCTV-6 ç”µå½±",
            "CCTV7": "CCTV-7 å›½é˜²å†›äº‹",
            "CCTV8": "CCTV-8 ç”µè§†å‰§",
            "CCTV9": "CCTV-9 çºªå½•",
            "CCTV11": "CCTV-11 æˆæ›²",
            "CCTV12": "CCTV-12 ç¤¾ä¼šä¸æ³•",
            "CCTV13": "CCTV-13 æ–°é—»",
            "CCTV14": "CCTV-14 å°‘å„¿",
            "CCTV15": "CCTV-15 éŸ³ä¹",
            "CCTV16": "CCTV-16 å¥¥æ—åŒ¹å…‹",
            "CCTV17": "CCTV-17 å†œä¸šå†œæ‘"
        }

        class ChannelProcessor:
            """ä¼˜åŒ–ç‰ˆçš„é¢‘é“å¤„ç†å™¨"""
            
            def __init__(self):
                self.valid_channels = []
                self.skipped_channels = []
                self.checked_channels = 0
                self.start_time = time.time()
            
            def _is_valid_stream_link(self, url):
                """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æµåª’ä½“é“¾æ¥æ ¼å¼"""
                # å¸¸è§çš„æµåª’ä½“åè®®
                stream_patterns = [
                    r'\.m3u8?$',  # m3u, m3u8
                    r'\.ts$',     # TSæµ
                    r'\.flv$',    # FLVæµ
                    r'rtmp://',   # RTMP
                    r'rtsp://',   # RTSP
                    r'rtp://',    # RTP
                    r'udp://',    # UDP
                    r'mms://',    # MMS
                    r'http://.*\.(ts|m3u8|flv)',  # HTTPæµ
                ]
                
                for pattern in stream_patterns:
                    if re.search(pattern, url, re.IGNORECASE):
                        return True
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºIP:ç«¯å£æ ¼å¼
                if re.search(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+', url):
                    return True
                
                return False
            
            def _should_skip_url(self, url):
                """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æŸä¸ªURLçš„æ£€æŸ¥"""
                # è·³è¿‡æ˜æ˜¾ä¸å¯ç”¨æˆ–æ ¼å¼é”™è¯¯çš„URL
                skip_patterns = [
                    r'^https?://$',  # ç©ºçš„URL
                    r'^//',         # åŒæ–œæ å¼€å¤´
                    r'^ftp://',     # FTPåè®®
                    r'^file://',    # æ–‡ä»¶åè®®
                    r'localhost',   # æœ¬åœ°åœ°å€
                    r'127\.0\.0\.1',  # ç¯å›åœ°å€
                    r'192\.168\.',  # å†…ç½‘åœ°å€
                    r'10\.',        # å†…ç½‘åœ°å€
                    r'172\.(1[6-9]|2[0-9]|3[0-1])\.',  # å†…ç½‘åœ°å€
                ]
                
                for pattern in skip_patterns:
                    if re.search(pattern, url, re.IGNORECASE):
                        return True
                
                return False
            
            def _extract_host_from_url(self, url):
                """ä»URLæå–ä¸»æœºå"""
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(url)
                    return parsed.hostname
                except:
                    return None
            
            async def _check_url_semaphore(self, session, semaphore, url, channel_name, hostname=None):
                """ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ£€æŸ¥URL"""
                async with semaphore:
                    return await self._check_single_url(session, url, channel_name, hostname)
            
            async def _check_single_url(self, session, url, channel_name, hostname=None):
                """æ£€æŸ¥å•ä¸ªURLçš„å¯ç”¨æ€§"""
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡
                if self._should_skip_url(url):
                    return False, channel_name, url, "æ— æ•ˆURLæ ¼å¼"
                
                # å¯¹äºæµåª’ä½“é“¾æ¥ï¼Œåªæ£€æŸ¥è¿æ¥æ€§ï¼Œä¸æ£€æŸ¥å†…å®¹
                is_stream = self._is_valid_stream_link(url)
                
                for attempt in range(MAX_RETRIES + 1):
                    try:
                        timeout = aiohttp.ClientTimeout(
                            connect=CONNECT_TIMEOUT,
                            sock_read=READ_TIMEOUT if not is_stream else 2,
                            total=TOTAL_TIMEOUT
                        )
                        
                        # å¯¹äºæµåª’ä½“é“¾æ¥ï¼Œä½¿ç”¨HEADæ–¹æ³•ï¼ˆæ›´å¿«ï¼‰
                        method = 'HEAD'
                        
                        async with session.request(
                            method, 
                            url, 
                            timeout=timeout,
                            allow_redirects=True,
                            headers={
                                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                                'Accept': '*/*',
                                'Connection': 'keep-alive'
                            }
                        ) as response:
                            # å¯¹äºæµåª’ä½“é“¾æ¥ï¼Œåªè¦ä¸æŠ›å‡ºå¼‚å¸¸å°±è®¤ä¸ºåŸºæœ¬å¯ç”¨
                            if is_stream:
                                # å°è¯•è¯»å–å°‘é‡æ•°æ®ç¡®è®¤æµå¯ç”¨
                                try:
                                    await response.read(100)
                                    return True, channel_name, url, f"æµåª’ä½“é“¾æ¥å¯ç”¨({response.status})"
                                except:
                                    # è¯»å–å¤±è´¥ï¼Œä½†è¿æ¥å»ºç«‹æˆåŠŸ
                                    if attempt < MAX_RETRIES:
                                        continue
                                    return False, channel_name, url, f"æµåª’ä½“è¿æ¥æˆåŠŸä½†è¯»å–å¤±è´¥"
                            
                            # å¯¹äºæ™®é€šé“¾æ¥ï¼Œæ£€æŸ¥çŠ¶æ€ç 
                            status = response.status
                            if status in VALID_STATUS_CODES:
                                return True, channel_name, url, f"HTTP {status}"
                            else:
                                if attempt < MAX_RETRIES:
                                    await asyncio.sleep(0.5)
                                    continue
                                return False, channel_name, url, f"HTTP {status}"
                                
                    except asyncio.TimeoutError:
                        if attempt < MAX_RETRIES:
                            await asyncio.sleep(0.5)
                            continue
                        return False, channel_name, url, f"è¶…æ—¶({TOTAL_TIMEOUT}s)"
                    except aiohttp.ClientConnectorError:
                        if attempt < MAX_RETRIES:
                            await asyncio.sleep(0.5)
                            continue
                        return False, channel_name, url, "è¿æ¥é”™è¯¯"
                    except aiohttp.ServerDisconnectedError:
                        if attempt < MAX_RETRIES:
                            await asyncio.sleep(1)
                            continue
                        return False, channel_name, url, "æœåŠ¡å™¨æ–­å¼€è¿æ¥"
                    except Exception as e:
                        error_msg = str(e)[:100]
                        if attempt < MAX_RETRIES:
                            await asyncio.sleep(0.5)
                            continue
                        return False, channel_name, url, f"å…¶ä»–é”™è¯¯: {error_msg}"
                
                return False, channel_name, url, "æ‰€æœ‰é‡è¯•å¤±è´¥"
            
            async def _batch_check_urls_with_host_limits(self, urls_info):
                """æ‰¹é‡æ£€æŸ¥URLï¼ŒæŒ‰ä¸»æœºåé™åˆ¶å¹¶å‘"""
                # æŒ‰ä¸»æœºååˆ†ç»„
                host_groups = {}
                for channel_name, url in urls_info:
                    hostname = self._extract_host_from_url(url)
                    if hostname not in host_groups:
                        host_groups[hostname] = []
                    host_groups[hostname].append((channel_name, url))
                
                # æŒ‰ä¸»æœºååˆ›å»ºä¿¡å·é‡ï¼Œé™åˆ¶æ¯ä¸ªä¸»æœºçš„å¹¶å‘æ•°
                semaphores = {}
                for hostname in host_groups:
                    semaphores[hostname] = asyncio.Semaphore(5)
                
                connector = aiohttp.TCPConnector(
                    limit_per_host=10,
                    limit=MAX_WORKERS * 2,
                    ttl_dns_cache=300
                )
                
                async with aiohttp.ClientSession(connector=connector) as session:
                    all_tasks = []
                    
                    for hostname, url_list in host_groups.items():
                        semaphore = semaphores.get(hostname, asyncio.Semaphore(5))
                        
                        for channel_name, url in url_list:
                            task = asyncio.create_task(
                                self._check_url_semaphore(session, semaphore, url, channel_name, hostname)
                            )
                            all_tasks.append(task)
                    
                    # æ”¶é›†ç»“æœ
                    results = []
                    total_tasks = len(all_tasks)
                    completed = 0
                    
                    # åˆ†æ‰¹æ”¶é›†ç»“æœï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
                    chunk_size = 50
                    for i in range(0, len(all_tasks), chunk_size):
                        chunk = all_tasks[i:i+chunk_size]
                        chunk_results = await asyncio.gather(*chunk, return_exceptions=True)
                        
                        for result in chunk_results:
                            completed += 1
                            if isinstance(result, Exception):
                                continue
                            results.append(result)
                        
                        # æ˜¾ç¤ºè¿›åº¦
                        progress = (completed / total_tasks) * 100
                        print(f"  â†³ é“¾æ¥æ£€æŸ¥è¿›åº¦: {progress:.1f}% ({completed}/{total_tasks})")
                    
                    return results
            
            def _check_source_content(self, content):
                """æ£€æŸ¥æºå†…å®¹æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„IP:ç«¯å£æ ¼å¼"""
                ip_port_pattern = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\:\d+'
                matches = re.findall(ip_port_pattern, content)
                match_count = len(matches)
                
                print(f"åœ¨æºå†…å®¹ä¸­æ‰¾åˆ° {match_count} ä¸ªIP:ç«¯å£æ ¼å¼çš„åŒ¹é…")
                
                if match_count < 10:
                    print(f"âŒ åŒ¹é…åˆ°çš„IP:ç«¯å£æ ¼å¼å°‘äº10ä¸ª({match_count})ï¼Œç»“æŸå¤„ç†")
                    return False
                else:
                    print(f"âœ… åŒ¹é…åˆ°çš„IP:ç«¯å£æ ¼å¼è¶³å¤Ÿ({match_count}ä¸ª)ï¼Œå¼€å§‹å¤„ç†")
                    return True
            
            def _normalize_tvname(self, tvname):
                """æ­¥éª¤80: æ ‡å‡†åŒ–tvname"""
                tvname = tvname.upper()
                characters_to_remove = [' ', '-', 'FHD', 'HD', 'SD', 'é«˜æ¸…', 'æ ‡æ¸…', 'è¶…æ¸…']
                for char in characters_to_remove:
                    tvname = tvname.replace(char, '')
                return tvname
            
            def _process_special_channels(self, tvname):
                """æ­¥éª¤100.1-100.3: å¤„ç†ç‰¹æ®Šé¢‘é“åç§°"""
                if "è¥¿è—å«è§†è—è¯­" in tvname:
                    return "è—è¯­å«è§†", "è—è¯­å«è§†"
                if tvname == "ç¬¬ä¸€è´¢ç»":
                    return "ä¸œæ–¹è´¢ç»", "ä¸œæ–¹è´¢ç»"
                if tvname == "ä¸­å›½äº¤é€šé¢‘é“":
                    return "ä¸­å›½äº¤é€š", "ä¸­å›½äº¤é€š"
                return tvname, tvname
            
            def _process_cgtn_channels(self, tvname):
                """æ­¥éª¤100.5: å¤„ç†CGTNé¢‘é“"""
                if "CGTNè‹±è¯­" in tvname:
                    return "CGTN", "CGTN"
                if "é˜¿æ‹‰ä¼¯è¯­" in tvname:
                    return "CGTNé˜¿è¯­", "CGTNé˜¿æ‹‰ä¼¯è¯­"
                if "è¥¿ç­ç‰™è¯­" in tvname:
                    return "CGTNè¥¿è¯­", "CGTNè¥¿ç­ç‰™è¯­"
                return tvname, tvname
            
            def _process_cctv_channels(self, tvname):
                """æ­¥éª¤100.7: å¤„ç†CCTVé¢‘é“"""
                if "3D" in tvname:
                    return "CCTV3D", "CCTV 3D"
                if "ç¾æ´²" in tvname or "AME" in tvname:
                    return "CCTV4ç¾æ´²", "CCTV-4 ç¾æ´²"
                if "æ¬§æ´²" in tvname or "EUO" in tvname:
                    return "CCTV4æ¬§æ´²", "CCTV-4 æ¬§æ´²"
                if "+" in tvname or "PLUS" in tvname:
                    return "CCTV5+", "CCTV-5+ ä½“è‚²èµ›äº‹"
                if "10" in tvname:
                    return "CCTV10", "CCTV-10 ç§‘æ•™"
                
                modified_name = tvname.replace("0", "") + "HD"
                pattern = r"(CCTV\d{1,2})[^\d]"
                match = re.search(pattern, modified_name)
                
                if match:
                    cctv_num = match.group(1)
                    display_name = CCTV_DISPLAY_NAMES.get(cctv_num, cctv_num)
                    return cctv_num, display_name
                
                return tvname, tvname
            
            def _build_logo_url(self, tvname):
                """æ„å»ºlogo URL"""
                safe_tvname = quote(tvname.replace("/", ""))
                return f"{LOGO_BASE_URL}{safe_tvname}.png"
            
            def _process_line(self, line, current_group_orig, current_group_cat):
                """å¤„ç†å•è¡Œæ•°æ®"""
                line = line.strip()
                if not line:
                    return current_group_orig, current_group_cat, None
                
                parts = line.split(',', 1)
                if len(parts) < 2:
                    return current_group_orig, current_group_cat, None
                
                str1, str2 = parts[0].strip(), parts[1].strip()
                
                if not str2:
                    return current_group_orig, current_group_cat, None
                elif str2 == "#genre#":
                    return str1, current_group_cat, None
                else:
                    tvname = str1
                    tvlink = str2
                    
                    if "4K" in tvname or "8K" in tvname or re.search(r"å«è§†[\d]", tvname):
                        return current_group_orig, current_group_cat, None
                    
                    tvname = self._normalize_tvname(tvname)
                    display_name = tvname
                    final_group_cat = current_group_cat
                    final_tvname = tvname
                    final_display_name = display_name
                    
                    final_tvname, final_display_name = self._process_special_channels(final_tvname)
                    
                    if "CETV" in final_tvname or "CGTN" in final_tvname:
                        final_group_cat = "å¤®è§†"
                        final_tvname, final_display_name = self._process_cgtn_channels(final_tvname)
                    elif "CCTV" in final_tvname:
                        final_group_cat = "å¤®è§†"
                        final_tvname, final_display_name = self._process_cctv_channels(final_tvname)
                    
                    logo_url = self._build_logo_url(final_tvname)
                    
                    return current_group_orig, current_group_cat, {
                        'tvname': final_tvname,
                        'display_name': final_display_name,
                        'tvlink': tvlink,
                        'group_orig': current_group_orig,
                        'group_cat': final_group_cat,
                        'group_flat': GROUP_FLAT,
                        'logo_url': logo_url,
                        'original_name': str1
                    }
            
            def _write_channel_entry(self, file_obj, tvname, group_name, display_name, url, logo_url):
                """å†™å…¥M3Uæ¡ç›®"""
                entry = f'#EXTINF:-1 tvg-name="{tvname}" '
                entry += f'tvg-logo="{logo_url}" '
                entry += f'group-title="{group_name}",{display_name}\n'
                entry += f"{url}\n"
                file_obj.write(entry)
            
            def _write_m3u_files(self):
                """å†™å…¥M3Uæ–‡ä»¶"""
                with (
                    open('JN950åŸç‰ˆ.m3u', 'w', encoding='utf-8') as f_orig,
                    open('JN950åˆ†èŠ‚ç›®.m3u', 'w', encoding='utf-8') as f_cat,
                    open('JN950æ— åˆ†ç»„.m3u', 'w', encoding='utf-8') as f_flat
                ):
                    m3u_header = f'#EXTM3U x-tvg-url="{EPG_URL}"\n'
                    f_orig.write(m3u_header)
                    f_cat.write(m3u_header)
                    f_flat.write(m3u_header)
                    
                    for channel_data in self.valid_channels:
                        self._write_channel_entry(
                            f_orig,
                            channel_data['tvname'],
                            channel_data['group_orig'],
                            channel_data['display_name'],
                            channel_data['tvlink'],
                            channel_data['logo_url']
                        )
                        
                        self._write_channel_entry(
                            f_cat,
                            channel_data['tvname'],
                            channel_data['group_cat'],
                            channel_data['display_name'],
                            channel_data['tvlink'],
                            channel_data['logo_url']
                        )
                        
                        self._write_channel_entry(
                            f_flat,
                            channel_data['tvname'],
                            channel_data['group_flat'],
                            channel_data['display_name'],
                            channel_data['tvlink'],
                            channel_data['logo_url']
                        )
                
                print(f"âœ… å·²ç”Ÿæˆ {len(self.valid_channels)} ä¸ªé¢‘é“çš„M3Uæ–‡ä»¶")
                total_time = time.time() - self.start_time
                print(f"ğŸ“Š æ€»è€—æ—¶: {total_time:.1f}ç§’, å¹³å‡: {total_time/max(len(self.valid_channels), 1):.2f}ç§’/é¢‘é“")
            
            async def process_channels_async(self):
                """å¼‚æ­¥å¤„ç†ä¸»å‡½æ•°"""
                try:
                    print("æ­£åœ¨è·å–æºå†…å®¹...")
                    response = requests.get(SOURCE_URL, timeout=10)
                    source_content = response.text
                    
                    if not self._check_source_content(source_content):
                        print("âŒ æºå†…å®¹ä¸ç¬¦åˆè¦æ±‚ï¼Œç»“æŸè„šæœ¬è¿è¡Œ")
                        return False
                    
                    print("è§£æé¢‘é“ä¿¡æ¯...")
                    all_parsed_channels = []
                    urls_to_check = []
                    
                    group_orig = ""
                    group_cat = "å«è§†"
                    
                    all_lines = source_content.splitlines()[2:]
                    
                    for line in all_lines:
                        result = self._process_line(line, group_orig, group_cat)
                        if result:
                            group_orig, group_cat, data = result
                            if data:
                                all_parsed_channels.append(data)
                                urls_to_check.append((data['original_name'], data['tvlink']))
                    
                    print(f"è§£æå‡º {len(all_parsed_channels)} ä¸ªé¢‘é“")
                    
                    if not all_parsed_channels:
                        print("âŒ æ²¡æœ‰è§£æåˆ°æœ‰æ•ˆé¢‘é“")
                        return False
                    
                    print(f"å¼€å§‹å¤šçº¿ç¨‹æ£€æŸ¥ {len(urls_to_check)} ä¸ªé“¾æ¥...")
                    print(f"é…ç½®: {MAX_WORKERS}ä¸ªå¹¶å‘, è¶…æ—¶: {TOTAL_TIMEOUT}s, é‡è¯•: {MAX_RETRIES}æ¬¡")
                    
                    valid_urls = set()
                    total_batches = (len(urls_to_check) + BATCH_SIZE - 1) // BATCH_SIZE
                    
                    for batch_idx in range(total_batches):
                        start_idx = batch_idx * BATCH_SIZE
                        end_idx = min((batch_idx + 1) * BATCH_SIZE, len(urls_to_check))
                        batch = urls_to_check[start_idx:end_idx]
                        
                        print(f"\næ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch)}ä¸ªé“¾æ¥)")
                        batch_start_time = time.time()
                        
                        results = await self._batch_check_urls_with_host_limits(batch)
                        
                        valid_count = 0
                        for result in results:
                            if isinstance(result, Exception):
                                continue
                            is_valid, channel_name, url, info = result
                            if is_valid:
                                valid_urls.add(url)
                                valid_count += 1
                            else:
                                self.skipped_channels.append((channel_name, url, info))
                        
                        batch_time = time.time() - batch_start_time
                        print(f"  ç»“æœ: {valid_count}/{len(batch)} ä¸ªæœ‰æ•ˆé“¾æ¥, è€—æ—¶: {batch_time:.1f}s")
                    
                    print(f"\nâœ… é“¾æ¥æ£€æŸ¥å®Œæˆ!")
                    print(f"   æœ‰æ•ˆé¢‘é“: {len(self.valid_channels)}")
                    print(f"   è·³è¿‡é¢‘é“: {len(self.skipped_channels)}")
                    
                    for channel_data in all_parsed_channels:
                        if channel_data['tvlink'] in valid_urls:
                            self.valid_channels.append(channel_data)
                    
                    if not self.valid_channels:
                        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é¢‘é“é“¾æ¥")
                        return False
                    
                    self._write_m3u_files()
                    return True
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                    return False

        def main():
            """ä¸»å‡½æ•°"""
            processor = ChannelProcessor()
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                success = loop.run_until_complete(processor.process_channels_async())
                if not success:
                    sys.exit(1)
            except KeyboardInterrupt:
                print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
                sys.exit(1)
            finally:
                loop.close()

        if __name__ == "__main__":
            main()
        EOF
        
        python tv_processor.py
      
    - name: å¼€å§‹æ¨é€
      if: success()  # åªæœ‰å‰é¢çš„æ­¥éª¤éƒ½æˆåŠŸæ‰æ‰§è¡Œæ¨é€
      run: |
        # é…ç½®Git
        git config --global user.name "Auto Sync"
        git config --global user.email "auto@sync.com"
        USER="tyrian9905"
        REPO="iptv"
        
        # è·å–åŒ—äº¬æ—¶é—´
        export TZ='Asia/Shanghai'
        BEIJING_TIME=$(date '+%Y-%m-%d %H:%M:%S')
        
        # å…‹éš†ä»“åº“
        git clone https://oauth2:${{ secrets.GITEE_TOKEN }}@gitee.com/${USER}/${REPO}.git
        
        # è¿›å…¥ä»“åº“å¹¶æ›´æ–°æ–‡ä»¶
        cd ${REPO}
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        mkdir -p iptv
        
        # æ£€æŸ¥M3Uæ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
        if [ ! -f "../JN950åŸç‰ˆ.m3u" ]; then
          echo "âŒ M3Uæ–‡ä»¶æœªç”Ÿæˆï¼Œè·³è¿‡æ¨é€"
          exit 0
        fi
        
        # å¤åˆ¶æ‰€æœ‰M3Uæ–‡ä»¶
        cp ../*.m3u ./ 2>/dev/null || true
        
        # æ·»åŠ ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
        git add .
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´
        if ! git diff --cached --quiet; then
          git commit -m "è‡ªåŠ¨åŒæ­¥æ–‡ä»¶ ${BEIJING_TIME}"
          git push origin master
          echo "âœ… æ–‡ä»¶å·²åŒæ­¥ï¼"
        else
          echo "âš ï¸ æ²¡æœ‰æ–‡ä»¶å˜æ›´ï¼Œè·³è¿‡æäº¤"
        fi
        
        echo "ğŸ‰ èŠ‚ç›®m3uæ›´æ–°æˆåŠŸï¼"
